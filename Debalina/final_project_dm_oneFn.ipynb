{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your team number (from the spreadsheet)]   \n",
    "[Your team names]   \n",
    "Summer 2019, section [Your section numbers>]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* __Section 1__ - Question Formulation\n",
    "* __Section 2__ - Algorithm Explanation\n",
    "* __Section 3__ - EDA & Challenges\n",
    "* __Section 4__ - Algorithm Implementation\n",
    "* __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 1__ - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 2__ - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 3__ - EDA & Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Initiate Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from numpy import allclose\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import  RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, VectorSlicer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session (RUN THIS CELL AS IS)\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw3_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(row, test_data = False, features=[]):\n",
    "    '''\n",
    "    Input: Take readable text row as input and transform it into RDD to be used for model.\n",
    "    test_data: a binary variable to indicate test or train data\n",
    "    Output: Two RDD-s\n",
    "    '''\n",
    "\n",
    "    #ALL HELPER FUNCTIONS\n",
    "\n",
    "\n",
    "    # Parse raw data\n",
    "    def parse_raw_row(row):\n",
    "        '''\n",
    "        for each row in the raw data,  output is a list of label and all the features:\n",
    "            - [label, feature_1, feature_1, ...]\n",
    "        For first 13 features, change the data type to number.\n",
    "        Remaining features will of type string.\n",
    "        For null values, populate None\n",
    "        '''\n",
    "\n",
    "        row_values = row.split('\\t')\n",
    "        for i, value in enumerate(row_values):\n",
    "            if i <14:\n",
    "                row_values[i] = float(value) if value != '' else None\n",
    "            else:\n",
    "                row_values[i] = value if value != '' else None\n",
    "        # \"''\"\n",
    "        return row_values\n",
    "\n",
    "    def imputeNumeric(numeric_DF, endCol):\n",
    "        '''\n",
    "        takes a spark df with continuous numeric columns\n",
    "        outputs a spark df where all null values are replaced with the column average\n",
    "\n",
    "        the first column, which is the outcome values, are preserved\n",
    "        '''\n",
    "        outputColumns=[\"{}_imputed\".format(c) for c in numeric_DF.columns[1:endCol]]\n",
    "        catColumns = [\"{}\".format(c) for c in numeric_DF.columns[endCol:]]\n",
    "\n",
    "        imputer = Imputer(\n",
    "            inputCols=numeric_DF.columns[1:endCol],\n",
    "            outputCols=outputColumns\n",
    "        )\n",
    "\n",
    "        model = imputer.fit(numeric_DF)\n",
    "\n",
    "        imputedDF = model.transform(numeric_DF).select(['_1']+outputColumns+catColumns)\n",
    "\n",
    "        return imputedDF\n",
    "\n",
    "    def scaleFeatures(imputedDF, endCol):\n",
    "        '''\n",
    "        inputs imputed data frame with no null values and continuous features\n",
    "        transforms the data frame into 2 column data frame with first column as label and second column as dense vector of features\n",
    "        scales all features using the StandardScalar\n",
    "        returns 2 column dataframe with scaled features\n",
    "        '''\n",
    "\n",
    "        transformedImputedDF = imputedDF.rdd.map(lambda x: (x[0], Vectors.dense(x[1:endCol]))).toDF(['label', 'x'])\n",
    "\n",
    "        scaler = StandardScaler(inputCol=\"x\",\n",
    "                            outputCol=\"features\",\n",
    "                            withStd=True, withMean=True)\n",
    "\n",
    "        scalerModel = scaler.fit(transformedImputedDF)\n",
    "        scaledDF = scalerModel.transform(transformedImputedDF).select(['label', 'features'])\n",
    "\n",
    "        return scaledDF\n",
    "\n",
    "\n",
    "    # Calculate click through rate frequency count of each category\n",
    "\n",
    "    def BinCategoricalFeatures(tenK_df4):\n",
    "        '''\n",
    "        takes a spark df with numerical and categorical columns\n",
    "        outputs a spark df where all the categorical features are binned using custom logic\n",
    "        '''\n",
    "        exclude_list = ['_20', '_31', '_37']\n",
    "\n",
    "        tenK_click_df = tenK_df4\n",
    "        for n,i in enumerate(tenK_df4.dtypes):\n",
    "\n",
    "            if i[1]=='string':\n",
    "\n",
    "                feature = i[0]\n",
    "\n",
    "                # frequency count of unique categories under each feature\n",
    "                cat_freqDF = tenK_df4.groupBy(feature).count()\n",
    "\n",
    "                # click through frequency count: count of 'label = 1' for each category\n",
    "                click_freqDF = tenK_df4.where(\"_1 == 1\").groupBy(feature, \"_1\").count()\n",
    "\n",
    "\n",
    "                ## Calculate click through frequency ratio for each category:\n",
    "                ##(count of 'label = 1'/total count)\n",
    "\n",
    "                df1 = click_freqDF.alias('df1')\n",
    "                df2 = cat_freqDF.alias('df2')\n",
    "                if n == 0:\n",
    "                    df3 = tenK_df4.alias('df3')\n",
    "                else:\n",
    "                    df3 = tenK_click_df.alias('df3')\n",
    "\n",
    "                tenK_click_df = df1.join(df2, [feature]).join(df3, [feature]).select(feature, 'df3.*',\n",
    "                                        (df1['count']/df2['count']).alias(feature+\"_click\"))\n",
    "\n",
    "                ## End of click through frequency ratio calculation\n",
    "\n",
    "                ###### Bin data into binary bins based on the click through rate(ctr).\n",
    "\n",
    "                if i[0] not in exclude_list:\n",
    "\n",
    "                    # if ctr == 0, value = A\n",
    "                    # else value = B\n",
    "                    # Keep null values as it is\n",
    "                    tenK_click_df = tenK_click_df.withColumn(feature,\n",
    "                    F.when(tenK_click_df[feature+'_click'] == 0, F.lit(\"A\"))\n",
    "                    .otherwise(F.lit(\"B\")))\n",
    "\n",
    "\n",
    "                elif i[0] in ['_20', '_31']:\n",
    "\n",
    "                    max_ctr = tenK_click_df.agg({feature+\"_click\": \"max\"}).collect()[0][0]\n",
    "                    ctr_threshold = max_ctr/2\n",
    "\n",
    "                    # if ctr == 0, value = A\n",
    "                    # if ctr > 0 and <= threshhold, value = B\n",
    "                    # else value = C\n",
    "                    # Keep null values as it is\n",
    "                    tenK_click_df = tenK_click_df.withColumn(feature,\n",
    "                    F.when(tenK_click_df[feature+'_click'] == 0, F.lit(\"A\"))\n",
    "                    .otherwise(\n",
    "                        F.when((tenK_click_df[feature+'_click'] > ctr_threshold)|(tenK_click_df[feature+'_click'] > ctr_threshold)\n",
    "                           , F.lit(\"B\"))\n",
    "                        .otherwise(F.lit(\"C\"))))\n",
    "\n",
    "                elif i[0] == '_37':\n",
    "\n",
    "                    max_ctr = tenK_click_df.agg({feature+\"_click\": \"max\"}).collect()[0][0]\n",
    "                    ctr_threshold1 = max_ctr/3\n",
    "                    ctr_threshold2 = 2*ctr_threshold1\n",
    "\n",
    "                    # if ctr == 0, value = A\n",
    "                    # if ctr > 0 and <= threshhold1, value = B\n",
    "                    # if ctr > threshhold1 and <= threshhold2, value = C\n",
    "                    # else value = D\n",
    "                    # Keep null values as it is\n",
    "\n",
    "                    tenK_click_df = tenK_click_df.withColumn(feature,\n",
    "                    F.when(tenK_click_df[feature+'_click'] == 0, F.lit(\"A\"))\n",
    "                    .otherwise(\n",
    "                        F.when(((tenK_click_df[feature+'_click'] > 0)\n",
    "                                & ((tenK_click_df[feature+'_click'] < ctr_threshold1) | (tenK_click_df[feature+'_click'] == ctr_threshold1)))\n",
    "                               , F.lit(\"B\"))\n",
    "                        .otherwise(\n",
    "                            F.when(((tenK_click_df[feature+'_click'] > ctr_threshold1)\n",
    "                                & ((tenK_click_df[feature+'_click'] < ctr_threshold2) | (tenK_click_df[feature+'_click'] == ctr_threshold2)))\n",
    "                               , F.lit(\"C\"))\n",
    "                            .otherwise(F.lit(\"D\")))))\n",
    "\n",
    "        tenK_df5 = tenK_click_df.drop('_15_click','_16_click','_19_click','_22_click','_25_click','_27_click',\n",
    "                                     '_28_click','_29_click', '_31_click', '_32_click', '_37_click', '_38_click'\n",
    "                                     ,'_20_click','_23_click','_31_click', '_37_click')\n",
    "\n",
    "        tenK_df5.cache()\n",
    "        return tenK_df5\n",
    "\n",
    "    # FeatureScore calculation using RandomForest Ensembling\n",
    "\n",
    "    def CalFeatureScore(tenK_df5):\n",
    "        '''\n",
    "        Takes input as a Spark DataFrame.\n",
    "        Fit and transfor using Assembler Pipeline\n",
    "        Run RandomForestClassifier to output top performing 30 features\n",
    "        '''\n",
    "\n",
    "        def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "            '''\n",
    "            Function to display featureImportances in human readable format\n",
    "            '''\n",
    "            list_extract = []\n",
    "            for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "                list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "            varlist = pd.DataFrame(list_extract)\n",
    "            varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "            return(varlist.sort_values('score', ascending = False))\n",
    "\n",
    "\n",
    "        encoding_var = [i[0] for i in tenK_df5.dtypes if (i[1]=='string')]\n",
    "        num_var = [i[0] for i in tenK_df5.dtypes if (i[1]!='string') & (i[0]!= '_1')]\n",
    "\n",
    "        string_indexes = [StringIndexer(inputCol = c, outputCol = 'IDX_' + c, handleInvalid = 'keep')\n",
    "                          for c in encoding_var]\n",
    "        onehot_indexes = [OneHotEncoderEstimator(inputCols = ['IDX_' + c], outputCols = ['OHE_' + c])\n",
    "                          for c in encoding_var]\n",
    "        label_indexes = StringIndexer(inputCol = '_1', outputCol = 'label', handleInvalid = 'keep')\n",
    "        assembler = VectorAssembler(inputCols = num_var + ['OHE_' + c for c in encoding_var]\n",
    "                                    , outputCol = \"features\")\n",
    "        rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,\n",
    "                                     numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)\n",
    "\n",
    "        pipe = Pipeline(stages = string_indexes + onehot_indexes + [assembler, label_indexes, rf])\n",
    "\n",
    "        ## fit into pipe\n",
    "\n",
    "        mod = pipe.fit(tenK_df5)\n",
    "        tenK_df6 = mod.transform(tenK_df5)\n",
    "\n",
    "        varlist = ExtractFeatureImp(mod.stages[-1].featureImportances, tenK_df6, \"features\")\n",
    "        top_features = [x for x in varlist['name'][0:30]]\n",
    "\n",
    "        return top_features\n",
    "\n",
    "    #Create data frame with one-hot encoding for categorical variables\n",
    "\n",
    "    def one_hot_encode(tenK_df5, top_features):\n",
    "        '''\n",
    "        Create data frame with one-hot encoding for categorical variables\n",
    "        Take input as Spark Data Frame\n",
    "        Output Spark DataFrame with hot-encoding\n",
    "        '''\n",
    "\n",
    "        one_hot = tenK_df5.toPandas()\n",
    "        encoding_var = [i[0] for i in tenK_df5.dtypes if (i[1]=='string')]\n",
    "        for col in encoding_var:\n",
    "            one_hot_pd = pd.concat([one_hot,pd.get_dummies(one_hot[col], prefix='OHE_'+col,dummy_na=False)],axis=1).drop([col],axis=1)\n",
    "            one_hot = one_hot_pd\n",
    "\n",
    "        one_hot_df = sqlContext.createDataFrame(one_hot_pd)\n",
    "\n",
    "        ###Keep the columns recommended by RandomForestClassifier\n",
    "\n",
    "        curr_col = one_hot_df.columns\n",
    "        col_to_drop = [x for x in curr_col if x not in top_features and x != '_1']\n",
    "\n",
    "        tenK_df7 = one_hot_df\n",
    "        for col in col_to_drop:\n",
    "            tenK_df7 = tenK_df7.drop(col)\n",
    "\n",
    "        return tenK_df7\n",
    "\n",
    "###########################################################\n",
    "    \n",
    "    # parse raw train data to form trainFeatureRDD\n",
    "    trainFeatureRDD = sc.parallelize(row).map(parse_raw_row).cache()\n",
    "    \n",
    "    ######### To run in GCP #######\n",
    "    \n",
    "    # parse raw data to form tenKRDD\n",
    "    #featureRDD = row.map(parse_raw_row).cache()\n",
    "    \n",
    "    #########\n",
    "\n",
    "    #### Create SQL dataframe from RDD\n",
    "    feature_df = sqlContext.createDataFrame(trainFeatureRDD)\n",
    "\n",
    "    # drop features with high unknown values\n",
    "    feature_df1 = feature_df.drop('_13','_36','_2','_11','_33','_34','_39','_40')\n",
    "\n",
    "    ### Remove Categorical features with high % of Uniqueness of Categories\n",
    "    feature_df2 = feature_df1.drop('_17','_18','_21','_24','_26','_30','_35')\n",
    "\n",
    "    ##Replace null with mean for numerical features\n",
    "    feature_df3 = imputeNumeric(feature_df2, 11)\n",
    "    feature_df3.cache()\n",
    "\n",
    "    #### Customize binning for categorical features\n",
    "    feature_df4 = BinCategoricalFeatures(feature_df3)\n",
    "\n",
    "    ### Use the top performing features recommended by RandomForest Classifier\n",
    "    #top_features = CalFeatureScore(feature_df4)\n",
    "\n",
    "    #### Call one-hot encoding\n",
    "    feature_df5 = one_hot_encode(feature_df4, top_features)\n",
    "\n",
    "    ####################################\n",
    "    #### Format data to be used in model\n",
    "\n",
    "    ### Build separate RDD for Categorical columns\n",
    "    catDF = feature_df5.select([c for c in feature_df5.columns if 'OHE' in c ])\n",
    "    catRDD = catDF.rdd\n",
    "\n",
    "    ### Standardize numerical column and Build separate RDD for Numerical columns\n",
    "    numericDF = scaleFeatures(feature_df5, 11)\n",
    "    numRDD = numericDF.rdd\n",
    "\n",
    "    ### Combine both the RDD-s to build full data RDD\n",
    "\n",
    "    FullDataRDD = numRDD.zip(catRDD)\n",
    "    FullDataRDD1 =  FullDataRDD.map(lambda x: (x[0][0], np.array(x[0][1]), np.array(x[1])))\\\n",
    "                               .map(lambda x: (x[0], np.append(x[1], x[2])))\n",
    "    FullDataRDD2 = FullDataRDD1.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "\n",
    "    print(FullDataRDD1.take(5))\n",
    "    print(FullDataRDD2.take(5))\n",
    "\n",
    "    return(FullDataRDD1, FullDataRDD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, array([-0.2713806 , -0.19770746, -0.70734428, -0.04519599,  2.25755412,\n",
      "       -0.23566383,  0.46222047,  2.36579736,  0.1294362 ,  0.01552995,\n",
      "        1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.26476714, -0.20692072, -0.84120987, -0.14951418, -0.38094305,\n",
      "       -0.08323582, -0.82944416,  0.17074711,  0.67525364, -0.73306695,\n",
      "        1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.09722618,  0.12475676,  0.01755856, -0.2379619 ,  0.04369815,\n",
      "       -0.36903833,  0.12230873, -0.46887018, -0.5983204 ,  0.05997833,\n",
      "        1.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
      "        0.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.20524601,  0.09020267,  0.01755856, -0.19090734, -0.41039996,\n",
      "        1.49820472, -0.8974265 , -0.12483361,  1.22107109,  0.05997833,\n",
      "        1.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (0.0, array([-0.25154023, -0.19770746,  0.36358038, -0.22914497, -0.34727802,\n",
      "       -0.33093133,  0.05432638, -0.4640246 , -0.41638125,  0.55024203,\n",
      "        1.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ]))]\n",
      "[(1.0, DenseVector([-0.2714, -0.1977, -0.7073, -0.0452, 2.2576, -0.2357, 0.4622, 2.3658, 0.1294, 0.0155, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.2648, -0.2069, -0.8412, -0.1495, -0.3809, -0.0832, -0.8294, 0.1707, 0.6753, -0.7331, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.0972, 0.1248, 0.0176, -0.238, 0.0437, -0.369, 0.1223, -0.4689, -0.5983, 0.06, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.2052, 0.0902, 0.0176, -0.1909, -0.4104, 1.4982, -0.8974, -0.1248, 1.2211, 0.06, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0])), (0.0, DenseVector([-0.2515, -0.1977, 0.3636, -0.2291, -0.3473, -0.3309, 0.0543, -0.464, -0.4164, 0.5502, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]))]\n",
      "[(1.0, array([ 0.369361  , -0.81823168, -0.54219947, -0.62585536, -0.57538662,\n",
      "        0.260045  ,  1.12142482,  1.02116058,  0.21705379, -0.59626435,\n",
      "        1.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  1.        ,  0.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.05832016,  0.92540787, -0.09923264,  0.26690175,  0.03544897,\n",
      "        0.01734854, -0.79479623,  0.0827029 , -0.12469048, -0.16344302,\n",
      "        1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  1.        ,  0.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.50686381,  1.48020228,  0.12595097, -0.80018429,  0.13355352,\n",
      "       -0.35223487,  1.12142482, -0.44524525, -0.3149589 ,  0.51958737,\n",
      "        1.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
      "        0.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  1.        ,  0.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.49643256,  0.0535881 ,  0.12595097,  0.25453586,  0.03544897,\n",
      "       -0.38446012,  1.12142482, -0.53429824, -0.49229647,  0.04136521,\n",
      "        1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  1.        ,  0.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.45470757,  0.13284444,  1.62928947, -0.64334858, -0.24937765,\n",
      "        0.51784705, -0.43550479,  1.76920565,  0.74906649,  2.75129081,\n",
      "        1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  1.        ,  0.        ,\n",
      "        1.        ,  1.        ,  1.        ]))]\n",
      "[(1.0, DenseVector([0.3694, -0.8182, -0.5422, -0.6259, -0.5754, 0.26, 1.1214, 1.0212, 0.2171, -0.5963, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.0583, 0.9254, -0.0992, 0.2669, 0.0354, 0.0173, -0.7948, 0.0827, -0.1247, -0.1634, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.5069, 1.4802, 0.126, -0.8002, 0.1336, -0.3522, 1.1214, -0.4452, -0.315, 0.5196, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.4964, 0.0536, 0.126, 0.2545, 0.0354, -0.3845, 1.1214, -0.5343, -0.4923, 0.0414, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.4547, 0.1328, 1.6293, -0.6433, -0.2494, 0.5178, -0.4355, 1.7692, 0.7491, 2.7513, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]))]\n"
     ]
    }
   ],
   "source": [
    "# Read the train and data file for feature extraction and data preparation\n",
    "rawTrainRDD = ast.literal_eval(open(\"data/train.txt\", \"r\").read())\n",
    "rawTestRDD = ast.literal_eval(open(\"data/test.txt\", \"r\").read())\n",
    "\n",
    "### In GCP ###\n",
    "# rawTrainRDD = sc.textFile('gs://w261-tktruong/data/train.txt')\n",
    "# rawTestRDD = sc.textFile('gs://w261-tktruong/data/test.txt')\n",
    "###########\n",
    "\n",
    "top_features = ['_7_imputed', '_14_imputed', '_6_imputed', '_8_imputed', '_9_imputed', '_10_imputed',\n",
    "                    '_12_imputed', '_3_imputed', 'OHE__31_B', '_5_imputed', '_4_imputed', 'OHE__31_C',\n",
    "                    'OHE__37_B', 'OHE__37_C', 'OHE__37_D', 'OHE__20_B', 'OHE__20_C', 'OHE__16_B',\n",
    "                    'OHE__19_B', 'OHE__23_B', 'OHE__22_B', 'OHE__32_B', 'OHE__25_B', 'OHE__27_B',\n",
    "                    'OHE__28_B', 'OHE__29_B', 'OHE__38_B', 'OHE__15_B']\n",
    "\n",
    "final_trainRDD1,final_trainRDD2 = feature_extraction(rawTrainRDD, features = top_features)\n",
    "final_testRDD1,final_testRDD2 = feature_extraction(rawTestRDD, features = top_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
