{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your team number (from the spreadsheet)]   \n",
    "[Your team names]   \n",
    "Summer 2019, section [Your section numbers>]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* __Section 1__ - Question Formulation\n",
    "* __Section 2__ - Algorithm Explanation\n",
    "* __Section 3__ - EDA & Challenges\n",
    "* __Section 4__ - Algorithm Implementation\n",
    "* __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 1__ - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 2__ - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 3__ - EDA & Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Initiate Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from numpy import allclose\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import  RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, VectorSlicer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session (RUN THIS CELL AS IS)\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw3_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup Path and read in raw data files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the toy data file\n",
    "toy_raw = ast.literal_eval(open(\"data/toy.txt\", \"r\").read())\n",
    "\n",
    "# Read the 10K sample data file for feature extraction and data preparation\n",
    "tenK_raw = ast.literal_eval(open(\"data/eda.txt\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_row(row):\n",
    "    '''\n",
    "    for each row in the raw data,  output is a list of label and all the features:\n",
    "        - [label, feature_1, feature_1, ...]\n",
    "    For first 13 features, change the data type to number.\n",
    "    Remaining features will of type string.\n",
    "    For null values, populate None\n",
    "    '''\n",
    "    row_values = row.split('\\t')\n",
    "    for i, value in enumerate(row_values):\n",
    "        if i <14:\n",
    "            row_values[i] = float(value) if value != '' else None\n",
    "        else:\n",
    "            row_values[i] = value if value != '' else None\n",
    "    # \"''\"\n",
    "    return row_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate click through rate frequency count of each category\n",
    "\n",
    "def BinCategoricalFeatures(tenK_df4):\n",
    "    '''\n",
    "    takes a spark df with numerical and categorical columns\n",
    "    outputs a spark df where all the categorical features are binned using custom logic\n",
    "    '''\n",
    "    exclude_list = ['_20', '_31', '_37']\n",
    "\n",
    "    tenK_click_df = tenK_df4\n",
    "    for n,i in enumerate(tenK_df4.dtypes):\n",
    "\n",
    "        if i[1]=='string':\n",
    "\n",
    "            feature = i[0]\n",
    "\n",
    "            # frequency count of unique categories under each feature\n",
    "            cat_freqDF = tenK_df4.groupBy(feature).count()\n",
    "\n",
    "            # click through frequency count: count of 'label = 1' for each category\n",
    "            click_freqDF = tenK_df4.where(\"_1 == 1\").groupBy(feature, \"_1\").count()\n",
    "\n",
    "\n",
    "            ## Calculate click through frequency ratio for each category:\n",
    "            ##(count of 'label = 1'/total count)\n",
    "\n",
    "            df1 = click_freqDF.alias('df1')\n",
    "            df2 = cat_freqDF.alias('df2')\n",
    "            if n == 0:\n",
    "                df3 = tenK_df4.alias('df3')\n",
    "            else:\n",
    "                df3 = tenK_click_df.alias('df3')\n",
    "\n",
    "            tenK_click_df = df1.join(df2, [feature]).join(df3, [feature]).select(feature, 'df3.*',\n",
    "                                    (df1['count']/df2['count']).alias(feature+\"_click\"))\n",
    "\n",
    "            ## End of click through frequency ratio calculation\n",
    "            \n",
    "            ###### Bin data into binary bins based on the click through rate(ctr).\n",
    "            \n",
    "            if i[0] not in exclude_list:\n",
    "\n",
    "                # if ctr == 0, value = A\n",
    "                # else value = B\n",
    "                # Keep null values as it is\n",
    "                tenK_click_df = tenK_click_df.withColumn(feature,\n",
    "                F.when(tenK_click_df[feature+'_click'] == 0, F.lit(\"A\"))\n",
    "                .otherwise(F.lit(\"B\")))\n",
    "\n",
    "\n",
    "            elif i[0] in ['_20', '_31']:\n",
    "                \n",
    "                max_ctr = tenK_click_df.agg({feature+\"_click\": \"max\"}).collect()[0][0]\n",
    "                ctr_threshold = max_ctr/2\n",
    "\n",
    "                # if ctr == 0, value = A\n",
    "                # if ctr > 0 and <= threshhold, value = B\n",
    "                # else value = C\n",
    "                # Keep null values as it is\n",
    "                tenK_click_df = tenK_click_df.withColumn(feature,\n",
    "                F.when(tenK_click_df[feature+'_click'] == 0, F.lit(\"A\"))\n",
    "                .otherwise(\n",
    "                    F.when((tenK_click_df[feature+'_click'] > ctr_threshold)|(tenK_click_df[feature+'_click'] > ctr_threshold)\n",
    "                       , F.lit(\"B\"))\n",
    "                    .otherwise(F.lit(\"C\"))))\n",
    "\n",
    "            elif i[0] == '_37':\n",
    "\n",
    "                max_ctr = tenK_click_df.agg({feature+\"_click\": \"max\"}).collect()[0][0]\n",
    "                ctr_threshold1 = max_ctr/3\n",
    "                ctr_threshold2 = 2*ctr_threshold1\n",
    "                \n",
    "                # if ctr == 0, value = A\n",
    "                # if ctr > 0 and <= threshhold1, value = B\n",
    "                # if ctr > threshhold1 and <= threshhold2, value = C\n",
    "                # else value = D\n",
    "                # Keep null values as it is\n",
    "                \n",
    "                tenK_click_df = tenK_click_df.withColumn(feature,\n",
    "                F.when(tenK_click_df[feature+'_click'] == 0, F.lit(\"A\"))\n",
    "                .otherwise(\n",
    "                    F.when(((tenK_click_df[feature+'_click'] > 0) \n",
    "                            & ((tenK_click_df[feature+'_click'] < ctr_threshold1) | (tenK_click_df[feature+'_click'] == ctr_threshold1)))\n",
    "                           , F.lit(\"B\"))\n",
    "                    .otherwise(\n",
    "                        F.when(((tenK_click_df[feature+'_click'] > ctr_threshold1) \n",
    "                            & ((tenK_click_df[feature+'_click'] < ctr_threshold2) | (tenK_click_df[feature+'_click'] == ctr_threshold2)))\n",
    "                           , F.lit(\"C\"))\n",
    "                        .otherwise(F.lit(\"D\")))))\n",
    "\n",
    "    tenK_df5 = tenK_click_df.drop('_15_click','_16_click','_19_click','_22_click','_25_click','_27_click',\n",
    "                                 '_28_click','_29_click', '_31_click', '_32_click', '_37_click', '_38_click'\n",
    "                                 ,'_20_click','_23_click','_31_click', '_37_click')\n",
    "\n",
    "    tenK_df5.cache()\n",
    "    return tenK_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureScore calculation using RandomForest Ensembling\n",
    "\n",
    "def CalFeatureScore(tenK_df5):\n",
    "    '''\n",
    "    Takes input as a Spark DataFrame.\n",
    "    Fit and transfor using Assembler Pipeline \n",
    "    Run RandomForestClassifier to output top performing 30 features\n",
    "    '''\n",
    "    \n",
    "    def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "        '''\n",
    "        Function to display featureImportances in human readable format\n",
    "        '''\n",
    "        list_extract = []\n",
    "        for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "            list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "        varlist = pd.DataFrame(list_extract)\n",
    "        varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "        return(varlist.sort_values('score', ascending = False))\n",
    "\n",
    "\n",
    "    encoding_var = [i[0] for i in tenK_df5.dtypes if (i[1]=='string')]\n",
    "    num_var = [i[0] for i in tenK_df5.dtypes if (i[1]!='string') & (i[0]!= '_1')]\n",
    "\n",
    "    string_indexes = [StringIndexer(inputCol = c, outputCol = 'IDX_' + c, handleInvalid = 'keep')\n",
    "                      for c in encoding_var]\n",
    "    onehot_indexes = [OneHotEncoderEstimator(inputCols = ['IDX_' + c], outputCols = ['OHE_' + c])\n",
    "                      for c in encoding_var]\n",
    "    label_indexes = StringIndexer(inputCol = '_1', outputCol = 'label', handleInvalid = 'keep')\n",
    "    assembler = VectorAssembler(inputCols = num_var + ['OHE_' + c for c in encoding_var]\n",
    "                                , outputCol = \"features\")\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,\n",
    "                                 numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)\n",
    "\n",
    "    pipe = Pipeline(stages = string_indexes + onehot_indexes + [assembler, label_indexes, rf])\n",
    "    \n",
    "    ## fit into pipe\n",
    "\n",
    "    mod = pipe.fit(tenK_df5)\n",
    "    tenK_df6 = mod.transform(tenK_df5)\n",
    "    \n",
    "    varlist = ExtractFeatureImp(mod.stages[-1].featureImportances, tenK_df6, \"features\")\n",
    "    top_features = [x for x in varlist['name'][0:30]]\n",
    "    \n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data frame with one-hot encoding for categorical variables\n",
    "\n",
    "def one_hot_encode(tenK_df5, top_features):\n",
    "    '''\n",
    "    Create data frame with one-hot encoding for categorical variables\n",
    "    Take input as Spark Data Frame\n",
    "    Output Spark DataFrame with hot-encoding\n",
    "    '''\n",
    "    \n",
    "    one_hot = tenK_df5.toPandas()\n",
    "    encoding_var = [i[0] for i in tenK_df5.dtypes if (i[1]=='string')]\n",
    "    for col in encoding_var:\n",
    "        one_hot_pd = pd.concat([one_hot,pd.get_dummies(one_hot[col], prefix='OHE_'+col,dummy_na=False)],axis=1).drop([col],axis=1)\n",
    "        one_hot = one_hot_pd\n",
    "\n",
    "    one_hot_df = spark.createDataFrame(one_hot_pd)\n",
    "\n",
    "    ###Keep the columns recommended by RandomForestClassifier\n",
    "\n",
    "    curr_col = one_hot_df.columns\n",
    "    col_to_drop = [x for x in curr_col if x not in top_features and x != '_1']\n",
    "\n",
    "    tenK_df7 = one_hot_df\n",
    "    for col in col_to_drop:\n",
    "        tenK_df7 = tenK_df7.drop(col)\n",
    "        \n",
    "    return tenK_df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use average imputer for null values\n",
    "\n",
    "def imputeNumeric(numeric_DF):\n",
    "    '''\n",
    "    takes a spark df with continuous numeric columns\n",
    "    outputs a spark df where all null values are replaced with the column average\n",
    "    \n",
    "    the first column, which is the outcome values, are preserved\n",
    "    '''\n",
    "    outputColumns=[\"{}\".format(c) for c in numeric_DF.columns[1:11]]\n",
    "    catColumns = [\"{}\".format(c) for c in numeric_DF.columns[11:]]\n",
    "    \n",
    "    imputer = Imputer(\n",
    "        inputCols=numeric_DF.columns[1:11], \n",
    "        outputCols=[\"{}\".format(c) for c in numeric_DF.columns[1:11]]\n",
    "    )\n",
    "\n",
    "    model = imputer.fit(numeric_DF)\n",
    "\n",
    "    imputedDF = model.transform(numeric_DF).select(['_1']+outputColumns+catColumns)\n",
    "\n",
    "    return imputedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleFeatures(inputedDF):\n",
    "    '''\n",
    "    inputs imputed data frame with no null values and continuous features\n",
    "    transforms the data frame into 2 column data frame with first column as label and second column as dense vector of features\n",
    "    scales all features using the StandardScalar\n",
    "    returns 2 column dataframe with scaled features\n",
    "    '''\n",
    "    \n",
    "    transformedImputedDF = inputedDF.rdd.map(lambda x: (x[0], Vectors.dense(x[1:11]))).toDF(['label', 'x'])\n",
    "    \n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"x\", \n",
    "                        outputCol=\"features\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "    scalerModel = scaler.fit(transformedImputedDF)\n",
    "    scaledDF = scalerModel.transform(transformedImputedDF).select(['label', 'features'])\n",
    "    \n",
    "    return scaledDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Row Into Readable formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse raw toy data to form toyRDD\n",
    "toyRDD = sc.parallelize(toy_raw).map(parse_raw_row).cache()\n",
    "\n",
    "# parse raw 10k sample data to form tenKRDD\n",
    "tenKRDD = sc.parallelize(tenK_raw).map(parse_raw_row).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe for Feature Engineering ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create SQL dataframe from RDD\n",
    "\n",
    "# for toy data\n",
    "toyFeature_df = sqlContext.createDataFrame(toyRDD)\n",
    "\n",
    "# for 10K sample data\n",
    "tenKfeature_df = sqlContext.createDataFrame(tenKRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1: Remove features with very large number of unknown data**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From EDA we see that the below feature columns have more than 40% null values.  \n",
    "Due to this high percenage of unknown data we won't keep these features in our model. So dropping these columns from our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----+---+----+---+----+----+---+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "| _1| _3| _4|  _5| _6|  _7| _8|  _9| _10|_12| _14|     _15|     _16|     _17|     _18|     _19|     _20|     _21|     _22|     _23|     _24|     _25|     _26|     _27|     _28|     _29|     _30|     _31|     _32|     _35|     _37|     _38|\n",
      "+---+---+---+----+---+----+---+----+----+---+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|0.0|6.0|3.0|34.0|4.0|27.0|3.0|33.0|34.0|1.0|15.0|05db9164|028bd518|77f2f2e5|d16679b9|25c83c98|fbad5c96|2ecb612f|5b392875|a73ee510|3b08e48b|4efc1873|9f32b866|f15f3681|b28479f6|9559bea6|31ca40b6|07c540c4|2a40f0da|dfcfc3fa|32c7478e|aee52b6f|\n",
      "+---+---+---+----+---+----+---+----+----+---+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop features with high unknown values\n",
    "\n",
    "toy_df1 = toyFeature_df.drop('_13','_36','_2','_11','_33','_34','_39','_40')\n",
    "tenK_df1 = tenKfeature_df.drop('_13','_36','_2','_11','_33','_34','_39','_40')\n",
    "\n",
    "tenK_df1.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Remove Categorical features with high % of Uniqueness of Categories**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From EDA, we see that for the following categorical features uniqueness is more than 50%. When uniqueness of a feature is more than 50% it should not be having much impact on label prediction. So will remove those columns from our model.  \n",
    "_17, _18, _21, _24, _26, _30, _35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+----+------+-----+----+----+-----+---+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "| _1|   _3|  _4|  _5|    _6|   _7|  _8|  _9|  _10|_12| _14|     _15|     _16|     _19|     _20|     _22|     _23|     _25|     _27|     _28|     _29|     _31|     _32|     _37|     _38|\n",
      "+---+-----+----+----+------+-----+----+----+-----+---+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|0.0|  6.0| 3.0|34.0|   4.0| 27.0| 3.0|33.0| 34.0|1.0|15.0|05db9164|028bd518|25c83c98|fbad5c96|5b392875|a73ee510|4efc1873|f15f3681|b28479f6|9559bea6|07c540c4|2a40f0da|32c7478e|aee52b6f|\n",
      "|0.0|521.0| 1.0| 2.0| 512.0| 21.0| 3.0|18.0| 44.0|1.0| 2.0|5bfa8ab5|38a947a1|25c83c98|    null|0b153874|a73ee510|b91c2548|a03da696|b28479f6|65afeec4|e5ba7672|b133fcd4|bcdee96c|8d365d3b|\n",
      "|1.0|  0.0| 1.0| 1.0|4982.0| null| 0.0| 1.0| 10.0|0.0| 1.0|5a9ed9b0|0b8e9caf|43b19349|fbad5c96|0b153874|a73ee510|064b8acf|45cbedc7|07d13a8f|1a015fe7|1e88c74f|ca6a63cf|423fab69|08b0ce98|\n",
      "|0.0|156.0|75.0|15.0|1352.0|276.0| 2.0|49.0|104.0|1.0|50.0|05db9164|38a947a1|25c83c98|7e0ccccf|5b392875|a73ee510|d059cd92|552e5180|b28479f6|eedd5e75|07c540c4|8e8e966f|32c7478e|0ff91809|\n",
      "|1.0|  0.0|12.0|12.0|   1.0|  0.0|16.0|12.0| 81.0|5.0| 0.0|68fd1e64|09e68b86|4cf72387|    null|0b153874|a73ee510|0bc63bd0|ef007ecc|07d13a8f|801ee1ae|e5ba7672|63cdbb21|32c7478e|1793a828|\n",
      "+---+-----+----+----+------+-----+----+----+-----+---+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toy_df2 = toy_df1.drop('_17','_18','_21','_24','_26','_30','_35')\n",
    "tenK_df2 = tenK_df1.drop('_17','_18','_21','_24','_26','_30','_35')\n",
    "tenK_df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Replace null values in numerical variables with mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----+---+----+---+----+----+---+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|_1 |_3 |_4 |_5  |_6 |_7  |_8 |_9  |_10 |_12|_14 |_15     |_16     |_19     |_20     |_22     |_23     |_25     |_27     |_28     |_29     |_31     |_32     |_37     |_38     |\n",
      "+---+---+---+----+---+----+---+----+----+---+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|0.0|6.0|3.0|34.0|4.0|27.0|3.0|33.0|34.0|1.0|15.0|05db9164|028bd518|25c83c98|fbad5c96|5b392875|a73ee510|4efc1873|f15f3681|b28479f6|9559bea6|07c540c4|2a40f0da|32c7478e|aee52b6f|\n",
      "+---+---+---+----+---+----+---+----+----+---+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Replace null with mean for numerical features\n",
    "\n",
    "tenK_df4 = imputeNumeric(tenK_df2)\n",
    "tenK_df4.cache()\n",
    "tenK_df4.show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4: Binning of Categories**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From EDA we reviewed the pattern of the distribution of each categorical feature. Depending on the type of distribution have binned the categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check frequency of unique categories of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+-----------------+-----------------+--------+------------------+----+----+------+----+-----------------+\n",
      "|_38|_37|_32|_31|_29|_28|_27|_25|_23|_22|_20|_19|_16|_15|_1 |_3   |_4               |_5               |_6      |_7                |_8  |_9  |_10   |_12 |_14              |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+-----------------+-----------------+--------+------------------+----+----+------+----+-----------------+\n",
      "|B  |D  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |1.0|0.0  |2.0              |2.0              |13140.0 |638.0             |7.0 |20.0|600.0 |4.0 |8.0              |\n",
      "|B  |D  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |1.0|3.0  |1.0              |1.0              |6396.0  |11.0              |15.0|1.0 |147.0 |7.0 |1.0              |\n",
      "|B  |B  |B  |C  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |1.0|79.0 |37.0             |7.415154749199573|678.0   |111.90972894482091|0.0 |15.0|15.0  |0.0 |8.415629076248074|\n",
      "|B  |C  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |1.0|30.0 |33.24952763344355|7.415154749199573|3720.0  |4.0               |98.0|0.0 |86.0  |10.0|8.415629076248074|\n",
      "|B  |C  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|9.0  |2.0              |10.0             |1248.0  |19.0              |2.0 |14.0|16.0  |1.0 |13.0             |\n",
      "|B  |D  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|10.0 |60.0             |20.0             |0.0     |177.0             |48.0|29.0|191.0 |9.0 |0.0              |\n",
      "|B  |D  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |1.0|215.0|33.24952763344355|1.0              |8.0     |0.0               |19.0|2.0 |144.0 |5.0 |0.0              |\n",
      "|B  |C  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|39.0 |35.0             |26.0             |0.0     |0.0               |70.0|20.0|610.0 |7.0 |0.0              |\n",
      "|B  |C  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |1.0|0.0  |6.0              |11.0             |285.0   |41.0              |10.0|20.0|367.0 |5.0 |29.0             |\n",
      "|B  |C  |B  |C  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |1.0|1.0  |1.0              |7.415154749199573|1707.0  |111.90972894482091|0.0 |2.0 |4.0   |0.0 |8.415629076248074|\n",
      "|B  |C  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|1.0  |7.0              |2.0              |387118.0|111.90972894482091|0.0 |2.0 |5.0   |0.0 |2.0              |\n",
      "|B  |C  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|4.0  |92.0             |5.0              |18108.0 |348.0             |1.0 |5.0 |214.0 |1.0 |5.0              |\n",
      "|B  |C  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|2.0  |13.0             |12.0             |46.0    |17.0              |5.0 |12.0|60.0  |2.0 |12.0             |\n",
      "|B  |C  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|1.0  |36.0             |4.0              |30.0    |69.0              |5.0 |14.0|50.0  |2.0 |4.0              |\n",
      "|B  |C  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|41.0 |10.0             |21.0             |1463.0  |315.0             |4.0 |22.0|208.0 |1.0 |35.0             |\n",
      "|B  |C  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|0.0  |33.24952763344355|7.415154749199573|14999.0 |59.0              |19.0|1.0 |59.0  |2.0 |8.415629076248074|\n",
      "|B  |D  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|0.0  |3.0              |1.0              |13135.0 |11.0              |1.0 |10.0|11.0  |1.0 |1.0              |\n",
      "|B  |D  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|1.0  |6.0              |4.0              |1500.0  |7.0               |3.0 |6.0 |7.0   |3.0 |4.0              |\n",
      "|B  |D  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|0.0  |2.0              |3.0              |10114.0 |238.0             |17.0|1.0 |1071.0|7.0 |8.0              |\n",
      "|B  |D  |B  |B  |B  |B  |B  |B  |B  |B  |C  |B  |B  |B  |0.0|2.0  |31.0             |37.0             |3121.0  |329.0             |1.0 |32.0|833.0 |1.0 |42.0             |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+-----------------+-----------------+--------+------------------+----+----+------+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Customize binning for categorical features\n",
    "\n",
    "tenK_df5 = BinCategoricalFeatures(tenK_df4)\n",
    "tenK_df5.show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5: Run RandomForest ensemble to check featureImportances**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With remaining features we will run RandomForest ensemble classifier to check featureImportances matrices.  \n",
    "We will extract the Features with higher featureImportances scores for our model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_7', '_14', '_6', '_8', '_9', '_10', '_12', '_3', 'OHE__31_B', '_5', '_4', 'OHE__31_C', 'OHE__37_B', 'OHE__37_C', 'OHE__37_D', 'OHE__20_B', 'OHE__20_C', 'OHE__16_B', 'OHE__19_B', 'OHE__23_B', 'OHE__22_B', 'OHE__32_B', 'OHE__25_B', 'OHE__27_B', 'OHE__28_B', 'OHE__29_B', 'OHE__38_B', 'OHE__15_B']\n"
     ]
    }
   ],
   "source": [
    "### Call RandomForest Classifier to retrieve top performing features\n",
    "top_features = CalFeatureScore(tenK_df5)\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. One hot-encoding of categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----------------+-----------------+-------+------------------+----+----+-----+----+-----------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|_1 |_3  |_4               |_5               |_6     |_7                |_8  |_9  |_10  |_12 |_14              |OHE__38_B|OHE__37_B|OHE__37_C|OHE__37_D|OHE__32_B|OHE__31_B|OHE__31_C|OHE__29_B|OHE__28_B|OHE__27_B|OHE__25_B|OHE__23_B|OHE__22_B|OHE__20_B|OHE__20_C|OHE__19_B|OHE__16_B|OHE__15_B|\n",
      "+---+----+-----------------+-----------------+-------+------------------+----+----+-----+----+-----------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|1.0|0.0 |2.0              |2.0              |13140.0|638.0             |7.0 |20.0|600.0|4.0 |8.0              |1        |0        |0        |1        |1        |1        |0        |1        |1        |1        |1        |1        |1        |0        |1        |1        |1        |1        |\n",
      "|1.0|3.0 |1.0              |1.0              |6396.0 |11.0              |15.0|1.0 |147.0|7.0 |1.0              |1        |0        |0        |1        |1        |1        |0        |1        |1        |1        |1        |1        |1        |0        |1        |1        |1        |1        |\n",
      "|1.0|79.0|37.0             |7.415154749199573|678.0  |111.90972894482091|0.0 |15.0|15.0 |0.0 |8.415629076248074|1        |1        |0        |0        |1        |0        |1        |1        |1        |1        |1        |1        |1        |0        |1        |1        |1        |1        |\n",
      "|1.0|30.0|33.24952763344355|7.415154749199573|3720.0 |4.0               |98.0|0.0 |86.0 |10.0|8.415629076248074|1        |0        |1        |0        |1        |1        |0        |1        |1        |1        |1        |1        |1        |0        |1        |1        |1        |1        |\n",
      "|0.0|9.0 |2.0              |10.0             |1248.0 |19.0              |2.0 |14.0|16.0 |1.0 |13.0             |1        |0        |1        |0        |1        |1        |0        |1        |1        |1        |1        |1        |1        |0        |1        |1        |1        |1        |\n",
      "+---+----+-----------------+-----------------+-------+------------------+----+----+-----+----+-----------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Call one-hot encoding\n",
    "\n",
    "tenK_df7 = one_hot_encode(tenK_df5, top_features)\n",
    "tenK_df7.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data to be used for Model Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data into numerical and categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(OHE__38_B=1, OHE__37_B=0, OHE__37_C=0, OHE__37_D=1, OHE__32_B=1, OHE__31_B=1, OHE__31_C=0, OHE__29_B=1, OHE__28_B=1, OHE__27_B=1, OHE__25_B=1, OHE__23_B=1, OHE__22_B=1, OHE__20_B=0, OHE__20_C=1, OHE__19_B=1, OHE__16_B=1, OHE__15_B=1),\n",
       " Row(OHE__38_B=1, OHE__37_B=0, OHE__37_C=0, OHE__37_D=1, OHE__32_B=1, OHE__31_B=1, OHE__31_C=0, OHE__29_B=1, OHE__28_B=1, OHE__27_B=1, OHE__25_B=1, OHE__23_B=1, OHE__22_B=1, OHE__20_B=0, OHE__20_C=1, OHE__19_B=1, OHE__16_B=1, OHE__15_B=1),\n",
       " Row(OHE__38_B=1, OHE__37_B=1, OHE__37_C=0, OHE__37_D=0, OHE__32_B=1, OHE__31_B=0, OHE__31_C=1, OHE__29_B=1, OHE__28_B=1, OHE__27_B=1, OHE__25_B=1, OHE__23_B=1, OHE__22_B=1, OHE__20_B=0, OHE__20_C=1, OHE__19_B=1, OHE__16_B=1, OHE__15_B=1),\n",
       " Row(OHE__38_B=1, OHE__37_B=0, OHE__37_C=1, OHE__37_D=0, OHE__32_B=1, OHE__31_B=1, OHE__31_C=0, OHE__29_B=1, OHE__28_B=1, OHE__27_B=1, OHE__25_B=1, OHE__23_B=1, OHE__22_B=1, OHE__20_B=0, OHE__20_C=1, OHE__19_B=1, OHE__16_B=1, OHE__15_B=1),\n",
       " Row(OHE__38_B=1, OHE__37_B=0, OHE__37_C=1, OHE__37_D=0, OHE__32_B=1, OHE__31_B=1, OHE__31_C=0, OHE__29_B=1, OHE__28_B=1, OHE__27_B=1, OHE__25_B=1, OHE__23_B=1, OHE__22_B=1, OHE__20_B=0, OHE__20_C=1, OHE__19_B=1, OHE__16_B=1, OHE__15_B=1)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Build separate RDD for Categorical columns\n",
    "\n",
    "catDF = tenK_df7.select([c for c in tenK_df7.columns if 'OHE' in c ])\n",
    "catRDD = catDF.rdd\n",
    "catRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize numerical data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([-0.2714, -0.1977, -0.7073, -0.0452, 2.2576, -0.2357, 0.4622, 2.3658, 0.1294, 0.0155])),\n",
       " Row(label=1.0, features=DenseVector([-0.2648, -0.2069, -0.8412, -0.1495, -0.3809, -0.0832, -0.8294, 0.1707, 0.6753, -0.7331])),\n",
       " Row(label=1.0, features=DenseVector([-0.0972, 0.1248, 0.0176, -0.238, 0.0437, -0.369, 0.1223, -0.4689, -0.5983, 0.06])),\n",
       " Row(label=1.0, features=DenseVector([-0.2052, 0.0902, 0.0176, -0.1909, -0.4104, 1.4982, -0.8974, -0.1248, 1.2211, 0.06])),\n",
       " Row(label=0.0, features=DenseVector([-0.2515, -0.1977, 0.3636, -0.2291, -0.3473, -0.3309, 0.0543, -0.464, -0.4164, 0.5502]))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Standardize numerical column and Build separate RDD for Numerical columns\n",
    "\n",
    "numericDF = scaleFeatures(tenK_df7)\n",
    "numRDD = numericDF.rdd\n",
    "numRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine both the RDD-s to build full data RDD\n",
    "\n",
    "FullDataRDD = numRDD.zip(catRDD)\n",
    "\n",
    "FullDataRDD1 =  FullDataRDD.map(lambda x: (x[0][0], np.array(x[0][1]), np.array(x[1])))\\\n",
    "                           .map(lambda x: (x[0], np.append(x[1], x[2])))\n",
    "\n",
    "FullDataRDD2 = FullDataRDD1.map(lambda x: (x[0],Vectors.dense(x[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, array([-0.2713806 , -0.19770746, -0.70734428, -0.04519599,  2.25755412,\n",
       "         -0.23566383,  0.46222047,  2.36579736,  0.1294362 ,  0.01552995,\n",
       "          1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
       "          1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        ])),\n",
       " (1.0, array([-0.26476714, -0.20692072, -0.84120987, -0.14951418, -0.38094305,\n",
       "         -0.08323582, -0.82944416,  0.17074711,  0.67525364, -0.73306695,\n",
       "          1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
       "          1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        ])),\n",
       " (1.0, array([-0.09722618,  0.12475676,  0.01755856, -0.2379619 ,  0.04369815,\n",
       "         -0.36903833,  0.12230873, -0.46887018, -0.5983204 ,  0.05997833,\n",
       "          1.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        ])),\n",
       " (1.0, array([-0.20524601,  0.09020267,  0.01755856, -0.19090734, -0.41039996,\n",
       "          1.49820472, -0.8974265 , -0.12483361,  1.22107109,  0.05997833,\n",
       "          1.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
       "          1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        ])),\n",
       " (0.0, array([-0.25154023, -0.19770746,  0.36358038, -0.22914497, -0.34727802,\n",
       "         -0.33093133,  0.05432638, -0.4640246 , -0.41638125,  0.55024203,\n",
       "          1.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
       "          1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        ]))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FullDataRDD1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,\n",
       "  DenseVector([-0.2714, -0.1977, -0.7073, -0.0452, 2.2576, -0.2357, 0.4622, 2.3658, 0.1294, 0.0155, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0])),\n",
       " (1.0,\n",
       "  DenseVector([-0.2648, -0.2069, -0.8412, -0.1495, -0.3809, -0.0832, -0.8294, 0.1707, 0.6753, -0.7331, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0])),\n",
       " (1.0,\n",
       "  DenseVector([-0.0972, 0.1248, 0.0176, -0.238, 0.0437, -0.369, 0.1223, -0.4689, -0.5983, 0.06, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0])),\n",
       " (1.0,\n",
       "  DenseVector([-0.2052, 0.0902, 0.0176, -0.1909, -0.4104, 1.4982, -0.8974, -0.1248, 1.2211, 0.06, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0])),\n",
       " (0.0,\n",
       "  DenseVector([-0.2515, -0.1977, 0.3636, -0.2291, -0.3473, -0.3309, 0.0543, -0.464, -0.4164, 0.5502, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "FullDataRDD2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Colenearity Reduction** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA shows strong correlation between below numerical features:  \n",
    "> _5 and _14  \n",
    "> _5 and _9  \n",
    "> _9 and _14  \n",
    "> _8 and _12  \n",
    "There is also a moderate negative correlation for feature:  \n",
    "> _6 and _11  \n",
    "> _7 and _11.  \n",
    "To avoid co-leniarity we will remove feature _14, _9, _6, _7 and _8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
