{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your team number (from the spreadsheet)]   \n",
    "[Your team names]   \n",
    "Summer 2019, section [Your section numbers>]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* __Section 1__ - Question Formulation\n",
    "* __Section 2__ - Algorithm Explanation\n",
    "* __Section 3__ - EDA & Challenges\n",
    "* __Section 4__ - Algorithm Implementation\n",
    "* __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 1__ - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 2__ - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 3__ - EDA & Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CTR data set being used here consists of millions of data row. Running any model on this large volume of data is anyway challenging. On top of that this data set has 26 categorical variables. And each categorical variable contains thousands of unique categories. If we simply use dummy variables or one-hot encoding for categorical variables, that will result in hundred-s of thousands of features making the model run logistically impossible.  \n",
    "Therefore, feature extraction has been the primary focus of our project.  \n",
    "We have followed below mentioned techniques to prepare our data and then extract features for model run. And the basis of applying these approaches is of course our EDA done on the data (mentioned in above section).  \n",
    "\n",
    ">**Remove features with very large number of unknown data**  \n",
    "\n",
    ">EDA shows that for features: 12, 35, 1, 10, 32, 33, 38, 39 more than 40% data are unknown. With this high percentage of unknown values these features can't be having much impact if used in our model. So we are not using these for our model building.  \n",
    "\n",
    ">**Remove Categorical features with high % of Uniqueness of Categories**  \n",
    "\n",
    ">Through EDA we calculated percentage of unique features for each of the categorical variable. If number of unique category is less, then each category will have more predictive power on unknown clicks. With same reasoning if a categorical variable has high number of unique values there is no point of using the variable in predicting unknown clicks.  \n",
    "For feature: 16, 17, 20, 23, 25, 29, 34 there are more than 50% unique categories, which is why we are removing these from our model.\n",
    "\n",
    ">**Handling of Null values**  \n",
    "\n",
    ">For numerical variables, we are replacing null values by mean of the entire data column.  \n",
    "For null values in categorical variables no action is taken. One-hot-encoding technique is applied on categorical variables with null values.  \n",
    "\n",
    ">**Normalization of Numerical Variables**  \n",
    "\n",
    "> To generalize numerical variables we are using StandardScaler method on each numerical column. The idea behind StandardScaler is that it will transform the data in such a way that its distribution will have a mean value 0 and standard deviation of 1. Given the distribution of the data, each value in the dataset will have the sample mean value subtracted, and then divided by the standard deviation of the whole dataset.  \n",
    "\n",
    ">**Binning of Categorical Variables**  \n",
    "\n",
    "> To handle categorical variables with large number of unique categories we are using Breimanâ€™s theorem. For every single categorical variable, first we are checking the the click through rate(CTR). By CTR we mean (Count of 1-s for the category)/(Total count for the category). Based on this matrice we are binning the categorical features.  \n",
    "To do that, in our EDA we have generated the histograms of CTR distribution for each categorical feature.  \n",
    ">1. For feature 14, 15, 18, 21, 22, 24, 26, 27, 28, 31, 37 the histagrams are spiked at zero, clearly recommending binary binning.  \n",
    ">2. For feature 19 and 30 histagram shows data are distributed mainly in three regions. So we are binning these two features in three bins in this way:\n",
    "if ctr == 0, value = A  \n",
    "if ctr > 0 and <= threshhold, value = B  \n",
    "else, value = C  \n",
    "Keep null values as it is.  \n",
    "\n",
    ">3. For feature 36, data are divided into four regions and so we are binning it into 4 bins with this logic:  \n",
    "if ctr > 0 and <= threshhold1, value = B  \n",
    "if ctr > threshhold1 and <= threshhold2, value = C  \n",
    "else, value = D  \n",
    "Keep null values as it is  \n",
    "\n",
    ">**One-hot encoding on Categorical Variables**  \n",
    "\n",
    ">As Machine learning algorithms cannot work with categorical data directly, categorical data must be converted to numbers. We are using one hot encoding to represent the categorical variables in binary vectors. Having our binning technique in place for categorical variables we have reasonably low number of unique categories to be represented through binary encoding.  \n",
    "After applying one-hot encoding, all our features are now numbers and can be used in machine learning model.  \n",
    "\n",
    ">**Random Forest Ensembling to retrieve FeatureScore**  \n",
    "\n",
    ">To get a sense of which of our variables (among 42 after one-hot encoding) have the most effect in machine learning models we ran Randome Forest ensemble methods on our latest data set.  Ensemble methods run many decision trees and aggregate their outputs for prediction. This method has a super helpful matrice called feature importances, which shows the feature score of each feature in the data set. Using this functionality we have chosen the final 28 features for our model.\n",
    "\n",
    ">Here is the list of twenty eight recommended features for our modelling:  \n",
    "Feature 6 (numerical)  \n",
    "Feature 13 (numerical)  \n",
    "Feature 5 (numerical)  \n",
    "Feature 7 (numerical)  \n",
    "Feature 8 (numerical)  \n",
    "Feature 9 (numerical)  \n",
    "Feature 11 (numerical)  \n",
    "Feature 2 (numerical)  \n",
    "Feature 4 (numerical)  \n",
    "Feature 3 (numerical)  \n",
    "Feature 30-bin2 (Categorical)  \n",
    "Feature 30-bin3 (Categorical)  \n",
    "Feature 36-bin2 (Categorical)  \n",
    "Feature 36-bin3 (Categorical)  \n",
    "Feature 36-bin4 (Categorical)    \n",
    "Feature 19-bin2 (Categorical)  \n",
    "Feature 19-bin3 (Categorical)  \n",
    "Feature 15-bin2 (Categorical)  \n",
    "Feature 18-bin2 (Categorical)  \n",
    "Feature 22-bin2 (Categorical)  \n",
    "Feature 21-bin2 (Categorical)  \n",
    "Feature 31-bin2 (Categorical)  \n",
    "Feature 24-bin2 (Categorical)  \n",
    "Feature 26-bin2 (Categorical)  \n",
    "Feature 27-bin2 (Categorical)  \n",
    "Feature 28-bin2 (Categorical)  \n",
    "Feature 37-bin2 (Categorical)  \n",
    "Feature 14-bin2 (Categorical)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Initiate Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from numpy import allclose\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import  RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, VectorSlicer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session (RUN THIS CELL AS IS)\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw3_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(row, test_data = False, features=[]):\n",
    "    '''\n",
    "    Input: Take readable text row as input and transform it into RDD to be used for model.\n",
    "    test_data: a binary variable to indicate test or train data\n",
    "    Output: Two RDD-s\n",
    "    '''\n",
    "\n",
    "    #ALL HELPER FUNCTIONS\n",
    "\n",
    "\n",
    "    # Parse raw data\n",
    "    def parse_raw_row(row):\n",
    "        '''\n",
    "        for each row in the raw data,  output is a list of label and all the features:\n",
    "            - [label, feature_1, feature_1, ...]\n",
    "        For first 13 features, change the data type to number.\n",
    "        Remaining features will of type string.\n",
    "        For null values, populate None\n",
    "        '''\n",
    "\n",
    "        row_values = row.split('\\t')\n",
    "        for i, value in enumerate(row_values):\n",
    "            if i <14:\n",
    "                row_values[i] = float(value) if value != '' else None\n",
    "            else:\n",
    "                row_values[i] = value if value != '' else None\n",
    "        # \"''\"\n",
    "        return row_values\n",
    "\n",
    "    def imputeNumeric(numeric_DF, endCol):\n",
    "        '''\n",
    "        takes a spark df with continuous numeric columns\n",
    "        outputs a spark df where all null values are replaced with the column average\n",
    "\n",
    "        the first column, which is the outcome values, are preserved\n",
    "        '''\n",
    "        outputColumns=[\"{}_imputed\".format(c) for c in numeric_DF.columns[1:endCol]]\n",
    "        catColumns = [\"{}\".format(c) for c in numeric_DF.columns[endCol:]]\n",
    "\n",
    "        imputer = Imputer(\n",
    "            inputCols=numeric_DF.columns[1:endCol],\n",
    "            outputCols=outputColumns\n",
    "        )\n",
    "\n",
    "        model = imputer.fit(numeric_DF)\n",
    "\n",
    "        imputedDF = model.transform(numeric_DF).select(['_1']+outputColumns+catColumns)\n",
    "\n",
    "        return imputedDF\n",
    "\n",
    "    def scaleFeatures(imputedDF, endCol):\n",
    "        '''\n",
    "        inputs imputed data frame with no null values and continuous features\n",
    "        transforms the data frame into 2 column data frame with first column as label and second column as dense vector of features\n",
    "        scales all features using the StandardScalar\n",
    "        returns 2 column dataframe with scaled features\n",
    "        '''\n",
    "\n",
    "        transformedImputedDF = imputedDF.rdd.map(lambda x: (x[0], Vectors.dense(x[1:endCol]))).toDF(['label', 'x'])\n",
    "\n",
    "        scaler = StandardScaler(inputCol=\"x\",\n",
    "                            outputCol=\"features\",\n",
    "                            withStd=True, withMean=True)\n",
    "\n",
    "        scalerModel = scaler.fit(transformedImputedDF)\n",
    "        scaledDF = scalerModel.transform(transformedImputedDF).select(['label', 'features'])\n",
    "\n",
    "        return scaledDF\n",
    "\n",
    "\n",
    "    # Calculate click through rate frequency count of each category\n",
    "\n",
    "    def BinCategoricalFeatures(tenK_df4):\n",
    "        '''\n",
    "        takes a spark df with numerical and categorical columns\n",
    "        outputs a spark df where all the categorical features are binned using custom logic\n",
    "        '''\n",
    "        exclude_list = ['_20', '_31', '_37']\n",
    "\n",
    "        tenK_click_df = tenK_df4\n",
    "        for n,i in enumerate(tenK_df4.dtypes):\n",
    "\n",
    "            if i[1]=='string':\n",
    "\n",
    "                feature = i[0]\n",
    "\n",
    "                # frequency count of unique categories under each feature\n",
    "                cat_freqDF = tenK_df4.groupBy(feature).count()\n",
    "\n",
    "                # click through frequency count: count of 'label = 1' for each category\n",
    "                click_freqDF = tenK_df4.where(\"_1 == 1\").groupBy(feature, \"_1\").count()\n",
    "\n",
    "\n",
    "                ## Calculate click through frequency ratio for each category:\n",
    "                ##(count of 'label = 1'/total count)\n",
    "\n",
    "                df1 = click_freqDF.alias('df1')\n",
    "                df2 = cat_freqDF.alias('df2')\n",
    "                if n == 0:\n",
    "                    df3 = tenK_df4.alias('df3')\n",
    "                else:\n",
    "                    df3 = tenK_click_df.alias('df3')\n",
    "\n",
    "                tenK_click_df = df1.join(df2, [feature]).join(df3, [feature]).select(feature, 'df3.*',\n",
    "                                        (df1['count']/df2['count']).alias(feature+\"_click\"))\n",
    "\n",
    "                ## End of click through frequency ratio calculation\n",
    "\n",
    "                ###### Bin data into binary bins based on the click through rate(ctr).\n",
    "\n",
    "                if i[0] not in exclude_list:\n",
    "\n",
    "                    # if ctr == 0, value = A\n",
    "                    # else value = B\n",
    "                    # Keep null values as it is\n",
    "                    tenK_click_df = tenK_click_df.withColumn(feature,\n",
    "                    F.when(tenK_click_df[feature+'_click'] == 0, F.lit(\"A\"))\n",
    "                    .otherwise(F.lit(\"B\")))\n",
    "\n",
    "\n",
    "                elif i[0] in ['_20', '_31']:\n",
    "\n",
    "                    max_ctr = tenK_click_df.agg({feature+\"_click\": \"max\"}).collect()[0][0]\n",
    "                    ctr_threshold = max_ctr/2\n",
    "\n",
    "                    # if ctr == 0, value = A\n",
    "                    # if ctr > 0 and <= threshhold, value = B\n",
    "                    # else value = C\n",
    "                    # Keep null values as it is\n",
    "                    tenK_click_df = tenK_click_df.withColumn(feature,\n",
    "                    F.when(tenK_click_df[feature+'_click'] == 0, F.lit(\"A\"))\n",
    "                    .otherwise(\n",
    "                        F.when((tenK_click_df[feature+'_click'] > ctr_threshold)|(tenK_click_df[feature+'_click'] > ctr_threshold)\n",
    "                           , F.lit(\"B\"))\n",
    "                        .otherwise(F.lit(\"C\"))))\n",
    "\n",
    "                elif i[0] == '_37':\n",
    "\n",
    "                    max_ctr = tenK_click_df.agg({feature+\"_click\": \"max\"}).collect()[0][0]\n",
    "                    ctr_threshold1 = max_ctr/3\n",
    "                    ctr_threshold2 = 2*ctr_threshold1\n",
    "\n",
    "                    # if ctr == 0, value = A\n",
    "                    # if ctr > 0 and <= threshhold1, value = B\n",
    "                    # if ctr > threshhold1 and <= threshhold2, value = C\n",
    "                    # else value = D\n",
    "                    # Keep null values as it is\n",
    "\n",
    "                    tenK_click_df = tenK_click_df.withColumn(feature,\n",
    "                    F.when(tenK_click_df[feature+'_click'] == 0, F.lit(\"A\"))\n",
    "                    .otherwise(\n",
    "                        F.when(((tenK_click_df[feature+'_click'] > 0)\n",
    "                                & ((tenK_click_df[feature+'_click'] < ctr_threshold1) | (tenK_click_df[feature+'_click'] == ctr_threshold1)))\n",
    "                               , F.lit(\"B\"))\n",
    "                        .otherwise(\n",
    "                            F.when(((tenK_click_df[feature+'_click'] > ctr_threshold1)\n",
    "                                & ((tenK_click_df[feature+'_click'] < ctr_threshold2) | (tenK_click_df[feature+'_click'] == ctr_threshold2)))\n",
    "                               , F.lit(\"C\"))\n",
    "                            .otherwise(F.lit(\"D\")))))\n",
    "\n",
    "        tenK_df5 = tenK_click_df.drop('_15_click','_16_click','_19_click','_22_click','_25_click','_27_click',\n",
    "                                     '_28_click','_29_click', '_31_click', '_32_click', '_37_click', '_38_click'\n",
    "                                     ,'_20_click','_23_click','_31_click', '_37_click')\n",
    "\n",
    "        tenK_df5.cache()\n",
    "        return tenK_df5\n",
    "\n",
    "    # FeatureScore calculation using RandomForest Ensembling\n",
    "\n",
    "    def CalFeatureScore(tenK_df5):\n",
    "        '''\n",
    "        Takes input as a Spark DataFrame.\n",
    "        Fit and transfor using Assembler Pipeline\n",
    "        Run RandomForestClassifier to output top performing 30 features\n",
    "        '''\n",
    "\n",
    "        def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "            '''\n",
    "            Function to display featureImportances in human readable format\n",
    "            '''\n",
    "            list_extract = []\n",
    "            for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "                list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "            varlist = pd.DataFrame(list_extract)\n",
    "            varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "            return(varlist.sort_values('score', ascending = False))\n",
    "\n",
    "\n",
    "        encoding_var = [i[0] for i in tenK_df5.dtypes if (i[1]=='string')]\n",
    "        num_var = [i[0] for i in tenK_df5.dtypes if (i[1]!='string') & (i[0]!= '_1')]\n",
    "\n",
    "        string_indexes = [StringIndexer(inputCol = c, outputCol = 'IDX_' + c, handleInvalid = 'keep')\n",
    "                          for c in encoding_var]\n",
    "        onehot_indexes = [OneHotEncoderEstimator(inputCols = ['IDX_' + c], outputCols = ['OHE_' + c])\n",
    "                          for c in encoding_var]\n",
    "        label_indexes = StringIndexer(inputCol = '_1', outputCol = 'label', handleInvalid = 'keep')\n",
    "        assembler = VectorAssembler(inputCols = num_var + ['OHE_' + c for c in encoding_var]\n",
    "                                    , outputCol = \"features\")\n",
    "        rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,\n",
    "                                     numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)\n",
    "\n",
    "        pipe = Pipeline(stages = string_indexes + onehot_indexes + [assembler, label_indexes, rf])\n",
    "\n",
    "        ## fit into pipe\n",
    "\n",
    "        mod = pipe.fit(tenK_df5)\n",
    "        tenK_df6 = mod.transform(tenK_df5)\n",
    "\n",
    "        varlist = ExtractFeatureImp(mod.stages[-1].featureImportances, tenK_df6, \"features\")\n",
    "        top_features = [x for x in varlist['name'][0:30]]\n",
    "\n",
    "        return top_features\n",
    "\n",
    "    #Create data frame with one-hot encoding for categorical variables\n",
    "\n",
    "    def one_hot_encode(tenK_df5, top_features):\n",
    "        '''\n",
    "        Create data frame with one-hot encoding for categorical variables\n",
    "        Take input as Spark Data Frame\n",
    "        Output Spark DataFrame with hot-encoding\n",
    "        '''\n",
    "\n",
    "        one_hot = tenK_df5.toPandas()\n",
    "        encoding_var = [i[0] for i in tenK_df5.dtypes if (i[1]=='string')]\n",
    "        for col in encoding_var:\n",
    "            one_hot_pd = pd.concat([one_hot,pd.get_dummies(one_hot[col], prefix='OHE_'+col,dummy_na=False)],axis=1).drop([col],axis=1)\n",
    "            one_hot = one_hot_pd\n",
    "\n",
    "        one_hot_df = sqlContext.createDataFrame(one_hot_pd)\n",
    "\n",
    "        ###Keep the columns recommended by RandomForestClassifier\n",
    "\n",
    "        curr_col = one_hot_df.columns\n",
    "        col_to_drop = [x for x in curr_col if x not in top_features and x != '_1']\n",
    "\n",
    "        tenK_df7 = one_hot_df\n",
    "        for col in col_to_drop:\n",
    "            tenK_df7 = tenK_df7.drop(col)\n",
    "\n",
    "        return tenK_df7\n",
    "\n",
    "###########################################################\n",
    "    \n",
    "    # parse raw train data to form trainFeatureRDD\n",
    "    trainFeatureRDD = sc.parallelize(row).map(parse_raw_row).cache()\n",
    "    \n",
    "    ######### To run in GCP #######\n",
    "    \n",
    "    # parse raw data to form tenKRDD\n",
    "    #featureRDD = row.map(parse_raw_row).cache()\n",
    "    \n",
    "    #########\n",
    "\n",
    "    #### Create SQL dataframe from RDD\n",
    "    feature_df = sqlContext.createDataFrame(trainFeatureRDD)\n",
    "\n",
    "    # drop features with high unknown values\n",
    "    feature_df1 = feature_df.drop('_13','_36','_2','_11','_33','_34','_39','_40')\n",
    "\n",
    "    ### Remove Categorical features with high % of Uniqueness of Categories\n",
    "    feature_df2 = feature_df1.drop('_17','_18','_21','_24','_26','_30','_35')\n",
    "\n",
    "    ##Replace null with mean for numerical features\n",
    "    feature_df3 = imputeNumeric(feature_df2, 11)\n",
    "    feature_df3.cache()\n",
    "\n",
    "    #### Customize binning for categorical features\n",
    "    feature_df4 = BinCategoricalFeatures(feature_df3)\n",
    "\n",
    "    ### Use the top performing features recommended by RandomForest Classifier\n",
    "    #top_features = CalFeatureScore(feature_df4)\n",
    "\n",
    "    #### Call one-hot encoding\n",
    "    feature_df5 = one_hot_encode(feature_df4, top_features)\n",
    "\n",
    "    ####################################\n",
    "    #### Format data to be used in model\n",
    "\n",
    "    ### Build separate RDD for Categorical columns\n",
    "    catDF = feature_df5.select([c for c in feature_df5.columns if 'OHE' in c ])\n",
    "    catRDD = catDF.rdd\n",
    "\n",
    "    ### Standardize numerical column and Build separate RDD for Numerical columns\n",
    "    numericDF = scaleFeatures(feature_df5, 11)\n",
    "    numRDD = numericDF.rdd\n",
    "\n",
    "    ### Combine both the RDD-s to build full data RDD\n",
    "\n",
    "    FullDataRDD = numRDD.zip(catRDD)\n",
    "    FullDataRDD1 =  FullDataRDD.map(lambda x: (x[0][0], np.array(x[0][1]), np.array(x[1])))\\\n",
    "                               .map(lambda x: (x[0], np.append(x[1], x[2])))\n",
    "    FullDataRDD2 = FullDataRDD1.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "\n",
    "    print(FullDataRDD1.take(5))\n",
    "    print(FullDataRDD2.take(5))\n",
    "\n",
    "    return(FullDataRDD1, FullDataRDD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, array([-0.2713806 , -0.19770746, -0.70734428, -0.04519599,  2.25755412,\n",
      "       -0.23566383,  0.46222047,  2.36579736,  0.1294362 ,  0.01552995,\n",
      "        1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.26476714, -0.20692072, -0.84120987, -0.14951418, -0.38094305,\n",
      "       -0.08323582, -0.82944416,  0.17074711,  0.67525364, -0.73306695,\n",
      "        1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.09722618,  0.12475676,  0.01755856, -0.2379619 ,  0.04369815,\n",
      "       -0.36903833,  0.12230873, -0.46887018, -0.5983204 ,  0.05997833,\n",
      "        1.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
      "        0.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.20524601,  0.09020267,  0.01755856, -0.19090734, -0.41039996,\n",
      "        1.49820472, -0.8974265 , -0.12483361,  1.22107109,  0.05997833,\n",
      "        1.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (0.0, array([-0.25154023, -0.19770746,  0.36358038, -0.22914497, -0.34727802,\n",
      "       -0.33093133,  0.05432638, -0.4640246 , -0.41638125,  0.55024203,\n",
      "        1.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  0.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ]))]\n",
      "[(1.0, DenseVector([-0.2714, -0.1977, -0.7073, -0.0452, 2.2576, -0.2357, 0.4622, 2.3658, 0.1294, 0.0155, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.2648, -0.2069, -0.8412, -0.1495, -0.3809, -0.0832, -0.8294, 0.1707, 0.6753, -0.7331, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.0972, 0.1248, 0.0176, -0.238, 0.0437, -0.369, 0.1223, -0.4689, -0.5983, 0.06, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.2052, 0.0902, 0.0176, -0.1909, -0.4104, 1.4982, -0.8974, -0.1248, 1.2211, 0.06, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0])), (0.0, DenseVector([-0.2515, -0.1977, 0.3636, -0.2291, -0.3473, -0.3309, 0.0543, -0.464, -0.4164, 0.5502, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]))]\n",
      "[(1.0, array([ 0.369361  , -0.81823168, -0.54219947, -0.62585536, -0.57538662,\n",
      "        0.260045  ,  1.12142482,  1.02116058,  0.21705379, -0.59626435,\n",
      "        1.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  1.        ,  0.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.05832016,  0.92540787, -0.09923264,  0.26690175,  0.03544897,\n",
      "        0.01734854, -0.79479623,  0.0827029 , -0.12469048, -0.16344302,\n",
      "        1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  1.        ,  0.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.50686381,  1.48020228,  0.12595097, -0.80018429,  0.13355352,\n",
      "       -0.35223487,  1.12142482, -0.44524525, -0.3149589 ,  0.51958737,\n",
      "        1.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
      "        0.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  1.        ,  0.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.49643256,  0.0535881 ,  0.12595097,  0.25453586,  0.03544897,\n",
      "       -0.38446012,  1.12142482, -0.53429824, -0.49229647,  0.04136521,\n",
      "        1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  1.        ,  0.        ,\n",
      "        1.        ,  1.        ,  1.        ])), (1.0, array([-0.45470757,  0.13284444,  1.62928947, -0.64334858, -0.24937765,\n",
      "        0.51784705, -0.43550479,  1.76920565,  0.74906649,  2.75129081,\n",
      "        1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
      "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ,  1.        ,  0.        ,\n",
      "        1.        ,  1.        ,  1.        ]))]\n",
      "[(1.0, DenseVector([0.3694, -0.8182, -0.5422, -0.6259, -0.5754, 0.26, 1.1214, 1.0212, 0.2171, -0.5963, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.0583, 0.9254, -0.0992, 0.2669, 0.0354, 0.0173, -0.7948, 0.0827, -0.1247, -0.1634, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.5069, 1.4802, 0.126, -0.8002, 0.1336, -0.3522, 1.1214, -0.4452, -0.315, 0.5196, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.4964, 0.0536, 0.126, 0.2545, 0.0354, -0.3845, 1.1214, -0.5343, -0.4923, 0.0414, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0])), (1.0, DenseVector([-0.4547, 0.1328, 1.6293, -0.6433, -0.2494, 0.5178, -0.4355, 1.7692, 0.7491, 2.7513, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]))]\n"
     ]
    }
   ],
   "source": [
    "# Read the train and data file for feature extraction and data preparation\n",
    "rawTrainRDD = ast.literal_eval(open(\"data/train.txt\", \"r\").read())\n",
    "rawTestRDD = ast.literal_eval(open(\"data/test.txt\", \"r\").read())\n",
    "\n",
    "### In GCP ###\n",
    "# rawTrainRDD = sc.textFile('gs://w261-tktruong/data/train.txt')\n",
    "# rawTestRDD = sc.textFile('gs://w261-tktruong/data/test.txt')\n",
    "###########\n",
    "\n",
    "top_features = ['_7_imputed', '_14_imputed', '_6_imputed', '_8_imputed', '_9_imputed', '_10_imputed',\n",
    "                    '_12_imputed', '_3_imputed', 'OHE__31_B', '_5_imputed', '_4_imputed', 'OHE__31_C',\n",
    "                    'OHE__37_B', 'OHE__37_C', 'OHE__37_D', 'OHE__20_B', 'OHE__20_C', 'OHE__16_B',\n",
    "                    'OHE__19_B', 'OHE__23_B', 'OHE__22_B', 'OHE__32_B', 'OHE__25_B', 'OHE__27_B',\n",
    "                    'OHE__28_B', 'OHE__29_B', 'OHE__38_B', 'OHE__15_B']\n",
    "\n",
    "final_trainRDD1,final_trainRDD2 = feature_extraction(rawTrainRDD, features = top_features)\n",
    "final_testRDD1,final_testRDD2 = feature_extraction(rawTestRDD, features = top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
